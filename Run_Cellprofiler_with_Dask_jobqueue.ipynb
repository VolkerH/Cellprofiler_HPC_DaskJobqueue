{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Cellprofiler Jobs on HPC clusters using `dask-jobqueue`\n",
    "\n",
    "Author: _Volker Hilsenstein_ (Monash Micro Imaging)\n",
    "\n",
    "### Aim:\n",
    "This Jupyter notebook will guide you step-by-step through the process of running a Cellprofiler pipeline in parallel on a High-Performance-Compute cluster. The python code for the widgets is hidden in seperate `.py` files that are imported. \n",
    "\n",
    "Here, we are running on Monash University's M3 _Massive_ cluster, which is running the SLURM scheduler. However,\n",
    "with minor changes this should also work on clusters running LSF, Moab, SGE or PBS if you change the `from dask_jobqueue import SLURMCluster as Cluster` statement  accordingly (see https://dask-jobqueue.readthedocs.io/en/latest/api.html).\n",
    "\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "If you have never used Jupyter notebooks before:\n",
    "In order to run this, go through the individual code cells (some of them have widgets), make changes if necessary, and execute the cells by pressing Shift+ENTER simultaneously. You will find many tutorials on Jupyter notebooks online.\n",
    "\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "* It is assumed that you have created a CellProfiler pipeline and that you have \n",
    "* It is assumed that Cellprofiler is installed on your cluster. This is a non-trivial step and you will probably need help of your HPC support team. If your cluster environment supports the Singularity container enviroment, you can find a build recipe for a container running CellProfiler 3.1.5 here: https://github.com/VolkerH/MyDockerStuff/tree/master/SingularityCellprofiler\n",
    "* You will have to set up a conda environment in your user-space on the cluster and set up an ssh tunnel. I recommend this video (https://www.youtube.com/watch?v=FXsgmwpRExM) by Matt Rocklin for guidance. Note that if you use `autossh` instead of `ssh` for setting up the tunnel, the tunnel will automatically be re-established if you intermittently use your network connection. This is useful if you use a laptop as your local machine, it will allow you to shut down, change WIFI networks and still keep the connection to your Jupyter notebook open. On Windows 10 you can run `autossh` easily if you install the windows subsystem for Linux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import  libraries\n",
    "just press Shift+Enter and wait until the cell has finished processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from dask_jobqueue import SLURMCluster as Cluster\n",
    "from dask import delayed\n",
    "from dask.distributed import Client, as_completed, progress\n",
    "from distributed.scheduler import KilledWorker\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from notebook_widgets import time_per_im, im_per_batch, batchfile, walltime_chooser\n",
    "from notebook_widgets import resource_chooser, gb_of_RAM, nr_cpus\n",
    "from notebook_widgets import FileBrowser\n",
    "from csv_concat import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed settings\n",
    "\n",
    "The following cell defines settings that are unlikely to change between different experiments but may have to be set once for each user (or system)\n",
    "\n",
    "## `cpexe`\n",
    "\n",
    "set this to the path of the cellprofiler executable (this might be a script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpexe = '/home/vhil0002/mycp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster options\n",
    "\n",
    "The High-Performance-Computing (HPC) environment at your institution may require parameters specific to that particular enviroment, such as queue names, project names etc.\n",
    "\n",
    "You can set such parameters in the configuration file `.config/dask/jobqueue.yaml` in your home directory (modify the file with a text editor. Refer to your local HPC instructions or support team to determine which parameters you need to set.)\n",
    "(Also see https://www.youtube.com/watch?v=FXsgmwpRExM from minute 11:20 onwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just have a look at our dask jobqueue settings\n",
    "!cat ~/.config/dask/jobqueue.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources (GPU, RAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_chooser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filebrowser, select Batchfile\n",
    "\n",
    "The following widget provides a file select box. Navigate to the `Batch_data.h5`.\n",
    "You generate this file by adding the `CreateBatchFiles` module to your Cellprofiler pipeline and running the pipeline once with the module enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchfile.widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchfile.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select batchsize and approximate processing time per image (for Walltime)\n",
    "\n",
    "The batchsize (images per batch) is the number of image set to be processed by each cluster job.\n",
    "\n",
    "The `walltime` is the time that a job gets allocated on the cluster. After the `walltime` has elapsed the job gets killed, no matter whether the Cellprofiler process has finished or not. A short `walltime` means that it will be easier to find the resources to squeeze a job in. You should try to set the `walltime` to the time that is needed for processing plus a little buffer time  in case the processing takes longer. Too large a value for walltime will mean that your jobs will take longer to be allocated.\n",
    "\n",
    "With the following sliders you can select the batchsize and your approximate Cellprofiler runtime estimate to process one image set. (To obtain such an estimate, run a few image sets in test mode, but note that processing times may differ for different image sets, e.g. due to different numbers of objects. So round up your estimate to allow for a buffer).\n",
    "\n",
    "The `walltime` will be calculated as the number of images per batch times the estimated processing time per image set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walltime_chooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walltime = time_per_im.value * im_per_batch.value\n",
    "print(\"Chosen walltime is\", walltime, \"minutes.\")\n",
    "print(\"If you want to change this value, just assign to the variable walltime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Batchfiles and list of commands\n",
    "\n",
    "Cellprofiler can generate its own list of batch commands when called with the options\n",
    "\n",
    "* `--get-batch-commands` followed by the path to the `Batchcommands.h5` file\n",
    "* `--images-per-batch` followed by the number of images that each seperate job should process.\n",
    "\n",
    "We run Cellprofiler with these options to create the list of shell commands. \n",
    "This may take a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [cpexe, '--get-batch-commands='+batchfile.path, '--images-per-batch='+str(im_per_batch.value)]\n",
    "print(\"Running  \"+\" \".join(cmd)+ \" to generate batch commands\")\n",
    "batchcmds = subprocess.check_output(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Substitute executable path\n",
    "As the commands generated by Cellprofiler assume that the Cellprofiler binary is called `CellProfiler`, we substitute this part of the command with the path to our Cellprofiler executable that is stored in the variable `cpexe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = batchcmds.decode(\"utf-8\").split(\"\\n\")\n",
    "# Some of the output lines should be ignored\n",
    "tmp = filter(lambda l: l.startswith(\"CellProfiler\"), tmp)\n",
    "# set correct executable\n",
    "cmds = [re.sub(r\"^CellProfiler\", cpexe,l) for l in tmp]\n",
    "cmds = [c.split(' ') for c in cmds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now create a cluster of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster=Cluster(cores=1, memory=str(gb_of_RAM.value)+\"GB\", projects='su62', walltime=str(walltime)+\":00\")\n",
    "cluster=Cluster(cores=1, memory=str(gb_of_RAM.value)+\"GB\", walltime=str(walltime)+\":00\", job_cpu=nr_cpus.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(len(cmds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask Dashboard\n",
    "see: https://github.com/VolkerH/Cellprofiler_HPC_DaskJobqueue/blob/master/Dashboard_via_nbserverproxy.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = client.map(lambda c: subprocess.check_output(c,stderr=subprocess.STDOUT), cmds[0:10])\n",
    "progress(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect output and shut down workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect results (cellprofiler output)\n",
    "res= []\n",
    "for future, result in as_completed(futures, with_results=True):\n",
    "    if future.exception():\n",
    "        print(future, \"produced an exception:\", future.exception())\n",
    "    print(future, \"finished\")\n",
    "    res.append(result)\n",
    "# now that we have all results we can close the cluster\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_CP_res(r):\n",
    "    print(r.decode('utf-8'))\n",
    "\n",
    "for r in res:\n",
    "    print_CP_res(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect and concatenate `.csv` files\n",
    "\n",
    "When running on a cluster, you will want to set up your `ExportToSpreadSheet` module in CellProfiler in such a\n",
    "way as to write a separate `.csv` file for each image set (or for each batch). This can be achieved by selecting output  `Elsewhere` for the output folder and using metadata field in the filename (right-click). \n",
    "\n",
    "The reason to save to seperate files is to prevent different cluster jobs trying to access the same files which may lead to prermission errors or possibly overwriting existing results.\n",
    "\n",
    "\n",
    "### Select the Folder below which the script should search for `.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_folder = FileBrowser(\"/scratch/su62/\")\n",
    "csv_folder.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below, select the folder where the merged `.csv` files should go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_csv_folder = FileBrowser(\"/scratch/su62/\")\n",
    "concat_csv_folder.widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the `.csv` merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_csvs(csv_folder.path, concat_csv_folder.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarks\n",
    "\n",
    "* Some tasks in this pipeline have been broken into several steps for the purpose of guiding you through the proces. In practice, you will probably want to combine several of the cells to automate things further. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
